# ğŸ§  AI Threat Modeling Gone Rogue â€“ Live Demo

This project demonstrates how semantic drift in architecture diagrams can break AI threat modelingâ€”and how to catch them using only open-source tools.

---

## âš™ï¸ 1. First-Time Setup (Fresh Clone)

Use the provided bootstrap script:

```bash
bash bootstrap.sh
```

This will:
- Create `.venv`
- Install all packages from `requirements.txt`
- Check for presence of a `.gguf` model

---

## â™»ï¸ 2. Reset Your Environment Anytime

To reset the environment and start fresh:

```bash
bash reset.sh
```

This will:
- Delete `.venv`, `baseline.json`, and `drift.json`
- Recreate the environment
- Reinstall all dependencies

---

## ğŸ“ 3. Place Your Llama Model

Ensure the `.gguf` model file is placed inside the `models/` directory.

You can use:
- `llama-3-8b-instruct.Q4_K_M.gguf` (recommended)
- `tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf` (faster fallback)

Then edit `scripts/generate_threat_model.py`:

```python
llm = LlamaCpp(
    model_path="models/your-model-name.gguf",
```

---

## ğŸš€ 4. Run the Demo

```bash
make baseline       # Generate threat model from baseline diagram
make drift          # Generate threat model from tampered diagram
make guardrails     # Validate drifted output using Guardrails
make demo           # Run the full pipeline
```

---

## ğŸ› ï¸ Common Fixes

- **Missing model file**: Confirm your `model_path` matches the `.gguf` file in `models/`.
- **LangChain 0.2+**: Requires `langchain_community` installed (included in `requirements.txt`).
- **Makefile errors**: Ensure lines are indented using **TAB**, not spaces.

---

## ğŸ“ Folder Structure

```
ai-drift-demo/
â”œâ”€â”€ diagrams/
â”œâ”€â”€ models/
â”œâ”€â”€ prompts/
â”œâ”€â”€ schema/
â”œâ”€â”€ scripts/
â”œâ”€â”€ baseline.json
â”œâ”€â”€ drift.json
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ reset.sh
â”œâ”€â”€ bootstrap.sh
â””â”€â”€ README.md
```

---

2025 â€“ Live Demo by Deepam Kanjani