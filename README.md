# ğŸ§  AI Threat Modeling Gone Rogue â€“ Live Demo

This project demonstrates how semantic drift in architecture diagrams can break AI threat modelingâ€”and how to catch it using only open-source tools.

---

## âš™ï¸ 1. Clean Setup Instructions

```bash
# OPTIONAL: Clean any old venv
rm -rf .venv

# Create new virtual environment
python3 -m venv .venv

# Activate it
source .venv/bin/activate   # or `. .venv/bin/activate`

# Install all required packages
pip install -r requirements.txt
```

---

## ğŸ“ 2. Place Your Llama Model

Ensure the `.gguf` model file is placed inside the `models/` directory.

You can use:
- `Meta-Llama-3-8B-Instruct-Q4_K_M.gguf` (recommended)
- `tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf` (faster fallback)

Check your model file path:

```bash
ls models/
```

Update this path in `scripts/generate_threat_model.py`:

```python
llm = LlamaCpp(
    model_path="models/your-model-name.gguf",
```

---

## ğŸš€ 3. Run the Demo

```bash
make baseline       # Generate threat model from baseline diagram
make drift          # Generate threat model from tampered diagram
make guardrails     # Validate drifted output using Guardrails
make demo           # Run the full pipeline
```

---

## ğŸ› ï¸ Common Fixes

- **Missing LLM model file**: Double-check `model_path` matches your `.gguf` filename.
- **LangChain 0.2+**: Requires `langchain_community` to be installed (included in `requirements.txt`).
- **Makefile error: missing separator**: Ensure lines are indented with **TAB**, not spaces.

---

## ğŸ“ Folder Structure

```
ai-drift-demo/
â”œâ”€â”€ diagrams/
â”œâ”€â”€ models/
â”œâ”€â”€ prompts/
â”œâ”€â”€ schema/
â”œâ”€â”€ scripts/
â”œâ”€â”€ baseline.json
â”œâ”€â”€ drift.json
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

2025 â€“ Live Demo by Deepam Kanjani
